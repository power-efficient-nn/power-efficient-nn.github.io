# Power-Efficient Neural Networks Using Low-Precision Data Types and Quantization

[Tutorial at CVPR 2025, Nashville, USA, on June 12.](https://cvpr.thecvf.com/virtual/2025/tutorial/35922)

## Abstract
The growing size of neural networks, particularly in generative AI, poses significant challenges in terms of sustainability, time, and cost, hindering their study and practical application. Low-precision data types and computations, especially when natively supported by hardware, offer an effective solution, enabling broader research access and deployment on edge devices. However, to this end networks that are usually trained with high-precision data types have to be prepared for low-precision execution. In this tutorial, we review different low-precision data types and showcase typical challenges of their application, like outlier handling, on simple hands-on examples. In order to maintain the original task performance of neural networks, sophisticated quantization methods are required to compensate for quantization errors induced by low-precision data types. We introduce and compare the most common and effective methods to quantize neural networks and provide guidance for practitioners.


## Schedule

- 1:00 pm - 1:10 pm: Opening remarks
- 1:10 pm - 2:10 pm: Session 1
  - Title: **Low-precision data types and computation**
  - Speaker: Thomas Pfeil
- 2:10 pm - 3:10 pm: Session 2
  - Title: **Quantization algorithms fundamentals**
  - Speaker: Markus Nagel
- 3:10 pm - 3:30 pm: Coffee break
- 3:30 pm - 4:30 pm: Session 3
  - Title: **Advanced LLM quantization methods**
  - Speaker: Tijmen Blankevoort

## Speaker

<table>
  <tr>
    <td  width="20%">
      <img src="https://media.licdn.com/dms/image/v2/C4E03AQEZqm9cU69qEA/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1607006591577?e=1754524800&v=beta&t=EoNXXm135cxH1dMIa51wlkTSbzhkqOhHMkeSmZgE33Q">
    </td>
    <td  width="20%">
      <img src="https://media.licdn.com/dms/image/v2/C4E03AQGvq0rqOpOBqw/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1516430227052?e=1754524800&v=beta&t=889tJ9oIVAQHBW0qUxkyeg4WKKFCW4aGVGlIdXrryfA">
    </td>
    <td  width="20%">
      <img src="https://media.licdn.com/dms/image/v2/D4E03AQGAZwkHCUrwIg/profile-displayphoto-shrink_800_800/B4EZOuD8d7HEAg-/0/1733792090802?e=1754524800&v=beta&t=mSdRQ8Ibm0vFRUCb2gG0nXDx_eoCmbDENJXF-nln__U">
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://www.linkedin.com/in/thomas-pfeil-6b0280a6/">Thomas Pfeil</a>
    </td>
    <td>
      <a href="https://www.linkedin.com/in/markus-nagel-2b820071/">Markus Nagel</a>
    </td>
    <td>
      <a href="https://www.linkedin.com/in/tijmen-blankevoort-a5633a24/">Tijmen Blankevoort</a>
    </td>
  </tr>
  <tr>
    <td>
      <a href="https://www.recogni.com/">Recogni</a>
    </td>
    <td>
      <a href="https://www.qualcomm.com/">Qualcomm</a>
    </td>
    <td>
      <a href="https://www.meta.com/">Meta</a>
    </td>
  </tr>
</table>
